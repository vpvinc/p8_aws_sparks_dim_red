{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b013ab-791a-4d18-8523-c67c3064c8a2",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b834c879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c557aa4678084f919fce3b206992ad23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1634031888152_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-9-47.us-east-2.compute.internal:20888/proxy/application_1634031888152_0004/\" class=\"emr-proxy-link\" emr-resource=\"j-1GJJ126UINT18\n",
       "\" application-id=\"application_1634031888152_0004\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-7-208.us-east-2.compute.internal:8042/node/containerlogs/container_1634031888152_0004_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42 # random state set for reproducibility\n",
    "\n",
    "# general modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "# spark modules\n",
    "# java 1.8.0_301\n",
    "# spark 3.1.2\n",
    "# hadoop 3.2.0\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# PCA ?\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import StringIndexer, StandardScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# modules for featurization with transfer learning\n",
    "from PIL import Image\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, split\n",
    "\n",
    "\n",
    "import boto3\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be312c3d-9e1c-47a0-be7d-058c552b5801",
   "metadata": {},
   "source": [
    "# Start and set up session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719787ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bdfce7d4364be4aabab6ece19b1bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# app name\n",
    "# master is automatically infered by AWS\n",
    "# access key for spark to connect s3\n",
    "# secret key for spark to connect s3\n",
    "# setting file system between hadoop and s3\n",
    "spark = SparkSession.builder.appName('p8') \\\n",
    "                            .config('spark.hadoop.fs.s3a.access.key', '<your access key>') \\\n",
    "                            .config('spark.hadoop.fs.s3a.secret.key', '<your secret key>') \\\n",
    "                            .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "                            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "# more settings for s3 and spark\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4eddb9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a32f3a7fb2840d4a982a42fc5918976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# connect to s3\n",
    "client = boto3.client('s3', region_name='us-east-2')\n",
    "# connexion check\n",
    "# bucket = client.list_objects(Bucket='s3-p8', Prefix='input_data_light/')\n",
    "# for content in bucket[\"Contents\"]:\n",
    "#    key = content[\"Key\"]\n",
    "#    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52489b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b385104310d43dfa39a1319010c400d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    }
   ],
   "source": [
    "# check that spark is running\n",
    "nums = sc.parallelize([1,2,3,4])\n",
    "nums.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6ceab3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54c887915cc4ecaa2ed25f2e6f46051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.driver.host', 'ip-172-31-9-47.us-east-2.compute.internal')\n",
      "('spark.eventLog.enabled', 'true')\n",
      "('spark.jars', 'file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.driver.appUIAddress', 'http://ip-172-31-9-47.us-east-2.compute.internal:4040')\n",
      "('spark.sql.parquet.output.committer.class', 'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter')\n",
      "('spark.blacklist.decommissioning.timeout', '1h')\n",
      "('spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS', '$(hostname -f)')\n",
      "('spark.ui.proxyBase', '/proxy/application_1634031888152_0004')\n",
      "('spark.sql.emr.internal.extensions', 'com.amazonaws.emr.spark.EmrSparkSessionExtensions')\n",
      "('spark.eventLog.dir', 'hdfs:///var/log/spark/apps')\n",
      "('spark.sql.warehouse.dir', 'hdfs:///user/spark/warehouse')\n",
      "('spark.app.name', 'livy-session-3')\n",
      "('spark.history.fs.logDirectory', 'hdfs:///var/log/spark/apps')\n",
      "('spark.ui.filters', 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter')\n",
      "('spark.pyspark.python', 'python3')\n",
      "('spark.yarn.historyServer.address', 'ip-172-31-9-47.us-east-2.compute.internal:18080')\n",
      "('spark.executor.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native')\n",
      "('spark.app.initial.jar.urls', 'spark://ip-172-31-9-47.us-east-2.compute.internal:40025/jars/commons-codec-1.9.jar,spark://ip-172-31-9-47.us-east-2.compute.internal:40025/jars/netty-all-4.1.17.Final.jar,spark://ip-172-31-9-47.us-east-2.compute.internal:40025/jars/livy-api-0.7.1-incubating.jar,spark://ip-172-31-9-47.us-east-2.compute.internal:40025/jars/livy-repl_2.12-0.7.1-incubating.jar,spark://ip-172-31-9-47.us-east-2.compute.internal:40025/jars/livy-core_2.12-0.7.1-incubating.jar,spark://ip-172-31-9-47.us-east-2.compute.internal:40025/jars/livy-rsc-0.7.1-incubating.jar,spark://ip-172-31-9-47.us-east-2.compute.internal:40025/jars/livy-thriftserver-session-0.7.1-incubating.jar')\n",
      "('spark.emr.maximizeResourceAllocation', 'true')\n",
      "('spark.hadoop.yarn.timeline-service.enabled', 'false')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.executorEnv.PYTHONPATH', '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip')\n",
      "('spark.driver.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar')\n",
      "('spark.hadoop.mapreduce.output.fs.optimized.committer.enabled', 'true')\n",
      "('spark.decommissioning.timeout.threshold', '20')\n",
      "('spark.sql.catalogImplementation', 'hive')\n",
      "('spark.default.parallelism', '16')\n",
      "('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true')\n",
      "('spark.yarn.tags', 'livy-session-3-b1k5Hno4')\n",
      "('spark.pyspark.virtualenv.enabled', 'true')\n",
      "('spark.yarn.dist.pyFiles', '')\n",
      "('spark.pyspark.virtualenv.bin.path', '/usr/bin/virtualenv')\n",
      "('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000')\n",
      "('spark.app.id', 'application_1634031888152_0004')\n",
      "('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem', '2')\n",
      "('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS', 'ip-172-31-9-47.us-east-2.compute.internal')\n",
      "('spark.yarn.submit.waitAppCompletion', 'false')\n",
      "('spark.yarn.secondary.jars', 'livy-api-0.7.1-incubating.jar,livy-rsc-0.7.1-incubating.jar,livy-thriftserver-session-0.7.1-incubating.jar,netty-all-4.1.17.Final.jar,commons-codec-1.9.jar,livy-core_2.12-0.7.1-incubating.jar,livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.pyspark.virtualenv.type', 'native')\n",
      "('spark.executor.cores', '4')\n",
      "('spark.app.startTime', '1634043815211')\n",
      "('spark.executor.memory', '10356M')\n",
      "('spark.yarn.dist.archives', 'file:/usr/lib/spark/R/lib/sparkr.zip#sparkr')\n",
      "('spark.driver.port', '40025')\n",
      "('spark.yarn.maxAppAttempts', '1')\n",
      "('spark.repl.class.outputDir', '/tmp/spark4063027323576013254')\n",
      "('spark.driver.memory', '11171M')\n",
      "('spark.repl.class.uri', 'spark://ip-172-31-9-47.us-east-2.compute.internal:40025/classes')\n",
      "('spark.sql.hive.metastore.sharedPrefixes', 'com.amazonaws.services.dynamodbv2')\n",
      "('spark.executor.instances', '2')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true')\n",
      "('spark.driver.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native')\n",
      "('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem', 'true')\n",
      "('spark.livy.spark_major_version', '3')\n",
      "('spark.executor.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar')\n",
      "('spark.repl.local.jars', 'file:///usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:///usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,file:///usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:///usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.history.ui.port', '18080')\n",
      "('spark.shuffle.service.enabled', 'true')\n",
      "('spark.driver.defaultJavaOptions', \"-XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.resourceManager.cleanupExpiredHost', 'true')\n",
      "('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES', 'http://ip-172-31-9-47.us-east-2.compute.internal:20888/proxy/application_1634031888152_0004')\n",
      "('spark.executor.defaultJavaOptions', \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.files.fetchFailure.unRegisterOutputOnHost', 'true')\n",
      "('spark.master', 'yarn')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.dynamicAllocation.enabled', 'true')\n",
      "('spark.yarn.isPython', 'true')\n",
      "('spark.yarn.dist.jars', 'file:///usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:///usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,file:///usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:///usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.blacklist.decommissioning.enabled', 'true')"
     ]
    }
   ],
   "source": [
    "# print spark config for checking\n",
    "configurations = spark.sparkContext.getConf().getAll()\n",
    "# for conf in configurations:\n",
    "#     print(conf)\n",
    "# spark.conf.get(\"spark.sql.files.maxRecordsPerFile\")\n",
    "# spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac558a99-3720-4820-9046-40345e9f3f2c",
   "metadata": {},
   "source": [
    "# Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61aa8fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276286247e7e4960b4a0ebb4ea551d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_url = \"s3a://s3-p8/input_data_light/*\"\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "            .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "            .option(\"recursiveFileLookup\", \"true\") \\\n",
    "            .load(s3_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6bdabac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f678097eb9b49dd9c2947b267e1f335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extracting fruit name from path\n",
    "images = images.withColumn('label', split(col('path'), '/').getItem(8))\n",
    "images = images.select('path', 'content', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead9ea9-91b8-41a7-91fc-742290312dce",
   "metadata": {},
   "source": [
    "# Featurization and transfer learning with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cba77c-e687-44fb-a71b-348a6f3d5ced",
   "metadata": {},
   "source": [
    "https://docs.databricks.com/applications/machine-learning/preprocess-data/transfer-learning-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67236e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb9daf33b034de3b784f90a9da4d3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verify that the top layer is removed\n",
    "model = ResNet50(include_top=False)\n",
    "# model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d2922e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cc57398e7f49e2994d3e7dbd108bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a ResNet50 model with top layer removed and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = ResNet50(weights=None, include_top=False)\n",
    "    model.set_weights(bc_model_weights.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1a9928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a004fac4cce41018dd377cfaaf2f5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(content):\n",
    "  \"\"\"\n",
    "  Preprocesses raw image bytes for prediction.\n",
    "  \"\"\"\n",
    "  img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "  arr = img_to_array(img)\n",
    "  return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "  \"\"\"\n",
    "  Featurize a pd.Series of raw images using the input model.\n",
    "  :return: a pd.Series of image features\n",
    "  \"\"\"\n",
    "  input = np.stack(content_series.map(preprocess))\n",
    "  preds = model.predict(input)\n",
    "  # For some layers, output features will be multi-dimensional tensors.\n",
    "  # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "  output = [p.flatten() for p in preds]\n",
    "  return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11f925b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0500f06fde46b0b3d2a7a70002d1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details."
     ]
    }
   ],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "  '''\n",
    "  This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "  The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "  \n",
    "  :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "  '''\n",
    "  # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "  # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "  model = model_fn()\n",
    "  for content_series in content_series_iter:\n",
    "    yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dafb9958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ac834782c74879812a1bd420a4a628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pandas UDFs on large records (e.g., very large images) can run into Out Of Memory (OOM) errors.\n",
    "# If you hit such errors in the cell below, try reducing the Arrow batch size via `maxRecordsPerBatch`.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27cfb6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59803250da0446e9738691388d2b5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can now run featurization on our entire Spark DataFrame.\n",
    "features_df = images.select(col(\"path\"), col(\"label\"), featurize_udf(\"content\").alias(\"features\"))\n",
    "# features_df.write.mode(\"overwrite\").parquet(\"C:/Users/VP/Documents/OC/P8/img_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f890589",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca2c1544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6793964b15ed45d8a075cf3c2dba4a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert features from list to vector.dense\n",
    "# it is done using a user defined function\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "features_df = features_df.select(col(\"path\"),  col(\"label\"), list_to_vector_udf(features_df[\"features\"]).alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d46954b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35502481acb84f45807c903d0fdc54ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# features have to be standardized for PCA\n",
    "standardizer = StandardScaler(withMean=True, withStd=True,\n",
    "                              inputCol='features',\n",
    "                              outputCol='feats_scaled')\n",
    "std = standardizer.fit(features_df)\n",
    "features_df_scaled = std.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fab6f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d147becd514ac79d97d282edbd3b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(k=7, inputCol=\"feats_scaled\", outputCol=\"pca\")\n",
    "modelpca = pca.fit(features_df_scaled)\n",
    "transformed = modelpca.transform(features_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13900cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828a92e4cf264e10a3edc25e6db57ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the spark dataframe to pd.DataFrame and save it as csv locally\n",
    "pd_transformed = transformed.select(\"path\", \"label\", \"pca\").toPandas()\n",
    "pd_transformed.to_csv(\"s3a://s3-p8/output_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
